"""
Enhanced context retrieval service for the RAG system
Implements multi-level context retrieval strategy with caching and ranking
"""

import asyncio
import hashlib
import json
import re
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime, timedelta
from sqlalchemy import select, and_, or_, desc, func
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.database import AsyncSessionLocal
from app.models.interaction import Interaction, Conversation
from app.models.context import (
    ConversationContext,
    UserLearningProfile,
    DocumentContext,
    ContextUsageLog,
)
from app.models.media import Media
from app.services.langchain_service import langchain_service
from app.services.cache_service import cache_service


class ContextRetrievalService:
    """Enhanced context retrieval service with multi-level strategy"""

    def __init__(self):
        self.cache_ttl = 900  # 15 minutes (increased for better performance)
        self.max_context_length = 10000  # Maximum context length to prevent token overflow (increased from 4000)

    async def get_comprehensive_context(
        self,
        user_id: str,
        interaction_id: Optional[str],
        message: str,
        include_cross_interaction: bool = True,
        max_context_length: int = None,
    ) -> Dict[str, Any]:
        """
        Get comprehensive context using multi-level retrieval strategy

        Returns:
            Dict with context from all sources and metadata about retrieval
        """
        start_time = datetime.now()
        max_context_length = max_context_length or self.max_context_length

        # Generate cache key
        cache_key = self._generate_cache_key(user_id, interaction_id, message)

        # Try cache first
        cached_context = await cache_service.get(cache_key)
        if cached_context:
            return cached_context

        context_sources = {
            "semantic_summary": "",
            "vector_search": [],
            "document_content": [],
            "cross_interaction": [],
            "related_conversations": [],
        }

        retrieval_metadata = {
            "sources_used": [],
            "sources_ignored": [],
            "retrieval_times": {},
            "context_lengths": {},
            "relevance_scores": {},
        }

        try:
            # === PARALLEL CONTEXT RETRIEVAL FOR SPEED ===
            # Run all context retrieval levels in parallel for much faster performance
            context_tasks = []

            # Level 1: Semantic summary
            context_tasks.append(
                self._get_semantic_summary_context(
                    user_id, interaction_id, retrieval_metadata
                )
            )

            # Level 2: Vector search
            context_tasks.append(
                self._get_vector_search_context(
                    user_id, interaction_id, message, retrieval_metadata
                )
            )

            # Level 3: Document content
            context_tasks.append(
                self._get_document_context(
                    user_id, interaction_id, message, retrieval_metadata
                )
            )

            # Level 4: Cross-interaction (if enabled)
            if include_cross_interaction:
                context_tasks.append(
                    self._get_cross_interaction_context(
                        user_id, interaction_id, message, retrieval_metadata
                    )
                )

            # Level 5: Related conversations
            context_tasks.append(
                self._get_related_conversations(
                    user_id, interaction_id, message, retrieval_metadata
                )
            )

            # Execute all context retrieval tasks in parallel
            context_results = await asyncio.gather(
                *context_tasks, return_exceptions=True
            )

            # Process results
            context_sources["semantic_summary"] = (
                context_results[0]
                if not isinstance(context_results[0], Exception)
                else {}
            )
            context_sources["vector_search"] = (
                context_results[1]
                if not isinstance(context_results[1], Exception)
                else {}
            )
            context_sources["document_content"] = (
                context_results[2]
                if not isinstance(context_results[2], Exception)
                else {}
            )

            if include_cross_interaction:
                context_sources["cross_interaction"] = (
                    context_results[3]
                    if not isinstance(context_results[3], Exception)
                    else {}
                )

            context_sources["related_conversations"] = (
                context_results[-1]
                if not isinstance(context_results[-1], Exception)
                else {}
            )

            # === CONTEXT RANKING AND OPTIMIZATION ===
            ranked_context = await self._rank_and_optimize_context(
                context_sources, message, max_context_length
            )

            # === BUILD FINAL CONTEXT ===
            final_context = self._build_final_context(ranked_context, message)

            # Calculate total retrieval time
            total_time = (datetime.now() - start_time).total_seconds()
            retrieval_metadata["total_retrieval_time"] = total_time

            # Cache the result
            result = {
                "context": final_context,
                "sources": context_sources,
                "metadata": retrieval_metadata,
                "context_length": len(final_context),
                "timestamp": datetime.now().isoformat(),
            }

            await cache_service.set(cache_key, result, self.cache_ttl)

            return result

        except Exception as e:
            # Log error and return minimal context
            retrieval_metadata["error"] = str(e)
            return {
                "context": "",
                "sources": context_sources,
                "metadata": retrieval_metadata,
                "context_length": 0,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    async def _get_semantic_summary_context(
        self, user_id: str, interaction_id: Optional[str], metadata: Dict
    ) -> str:
        """Get interaction-level semantic summary context"""
        start_time = datetime.now()

        try:
            if not interaction_id:
                return ""

            async with AsyncSessionLocal() as db:
                result = await db.execute(
                    select(Interaction).where(
                        and_(
                            Interaction.id == interaction_id,
                            Interaction.user_id == user_id,
                        )
                    )
                )
                interaction = result.scalar_one_or_none()

                if not interaction or not interaction.semantic_summary:
                    return ""

                summary_data = interaction.semantic_summary
                semantic_parts = []

                # Add the main summary
                if summary_data.get("updated_summary"):
                    semantic_parts.append(
                        f"**Conversation Summary:**\n{summary_data['updated_summary']}"
                    )

                # Add recent focus (most important for understanding current context)
                if summary_data.get("recent_focus"):
                    semantic_parts.append(
                        f"**Recent Focus:**\n{summary_data['recent_focus']}"
                    )

                # Add key topics
                if summary_data.get("key_topics"):
                    topics_str = ", ".join(summary_data["key_topics"][:5])
                    semantic_parts.append(f"**Topics Covered:** {topics_str}")

                # Add accumulated facts (critical for answering follow-up questions)
                if summary_data.get("accumulated_facts"):
                    facts = summary_data["accumulated_facts"][
                        :3
                    ]  # Top 3 most important
                    if facts:
                        facts_str = "\n".join(f"- {fact}" for fact in facts)
                        semantic_parts.append(f"**Key Facts:**\n{facts_str}")

                context = "\n\n".join(semantic_parts)

                # Update metadata
                metadata["sources_used"].append("semantic_summary")
                metadata["retrieval_times"]["semantic_summary"] = (
                    datetime.now() - start_time
                ).total_seconds()
                metadata["context_lengths"]["semantic_summary"] = len(context)

                return context

        except Exception as e:
            metadata["sources_ignored"].append("semantic_summary")
            metadata["retrieval_times"]["semantic_summary"] = (
                datetime.now() - start_time
            ).total_seconds()
            return ""

    async def _get_vector_search_context(
        self, user_id: str, interaction_id: Optional[str], message: str, metadata: Dict
    ) -> List[Dict[str, Any]]:
        """Get context from vector search using enhanced optimization service"""
        start_time = datetime.now()

        try:
            # Use the new vector optimization service for better results
            from app.services.vector_optimization_service import (
                vector_optimization_service,
                SearchQuery,
            )

            # Build enhanced search query
            search_query = SearchQuery(
                query=message,
                user_id=user_id,
                interaction_id=interaction_id,
                top_k=8 if interaction_id else 10,
                use_hybrid_search=True,
                use_query_expansion=True,
                boost_recent=True,
                boost_interaction=bool(interaction_id),
            )

            # Get enhanced search results
            search_results = await vector_optimization_service.hybrid_search(
                search_query
            )

            # Convert SearchResult objects to expected format
            results = []
            for result in search_results:
                # Truncate content for faster processing
                content = result.content
                if len(content) > 300:
                    content = content[:300] + "..."

                results.append(
                    {
                        "id": result.id,
                        "interaction_id": result.interaction_id,
                        "title": result.title,
                        "content": content,
                        "metadata": result.metadata,
                        "score": result.score,
                        "type": result.content_type,
                        "relevance_score": result.relevance_score,
                        "recency_score": result.recency_score,
                        "importance_score": result.importance_score,
                        "topic_tags": result.topic_tags,
                        "question_numbers": result.question_numbers,
                    }
                )

            # Filter and rank results
            filtered_results = []
            for result in results:
                if result.get("content") and len(result["content"].strip()) > 10:
                    # Add relevance score if not present
                    if "score" not in result:
                        result["score"] = 0.8  # Default score

                    # Boost score for interaction-specific results
                    if result.get("interaction_id") == interaction_id:
                        result["score"] = min(1.0, result["score"] + 0.2)

                    filtered_results.append(result)

            # Sort by score and limit
            filtered_results.sort(key=lambda x: x.get("score", 0), reverse=True)
            final_results = filtered_results[:5]

            # Update metadata
            metadata["sources_used"].append("vector_search")
            metadata["retrieval_times"]["vector_search"] = (
                datetime.now() - start_time
            ).total_seconds()
            metadata["context_lengths"]["vector_search"] = sum(
                len(r.get("content", "")) for r in final_results
            )
            metadata["relevance_scores"]["vector_search"] = [
                r.get("score", 0) for r in final_results
            ]

            return final_results

        except Exception as e:
            metadata["sources_ignored"].append("vector_search")
            metadata["retrieval_times"]["vector_search"] = (
                datetime.now() - start_time
            ).total_seconds()
            return []

    async def _get_document_context(
        self, user_id: str, interaction_id: Optional[str], message: str, metadata: Dict
    ) -> List[Dict[str, Any]]:
        """Get context from uploaded documents using enhanced document integration service"""
        start_time = datetime.now()

        try:
            # Use the new document integration service for better document search
            from app.services.document_integration_service import (
                document_integration_service,
            )

            # Extract question numbers from the message for targeted search
            question_numbers = self._extract_question_numbers(message)

            # AGGRESSIVE APPROACH: Always try to get recent document content first
            document_results = []

            # 1. If user is asking about specific questions, try to get those directly
            if question_numbers and interaction_id:
                for q_num in question_numbers:
                    doc_content = await document_integration_service.get_document_by_question_number(
                        user_id, interaction_id, q_num
                    )
                    if doc_content:
                        document_results.append(
                            {
                                "id": f"doc_q_{q_num}",
                                "media_id": f"question_{q_num}",
                                "document_type": doc_content.get(
                                    "document_type", "unknown"
                                ),
                                "content": f"Question {q_num}: {doc_content['question_text']}\n\nAnswer: {doc_content.get('answer', 'Not provided')}",
                                "main_topics": doc_content.get("main_topics", []),
                                "total_questions": 1,
                                "relevance_score": 0.95,  # Very high relevance for specific questions
                                "question_number": q_num,
                                "subject_area": doc_content.get(
                                    "subject_area", "unknown"
                                ),
                                "difficulty_level": doc_content.get(
                                    "difficulty_level", "unknown"
                                ),
                                "key_concepts": doc_content.get("key_concepts", []),
                            }
                        )

            # 2. AGGRESSIVE FALLBACK: If no specific questions found OR user is asking general questions,
            # get ALL document content from the current interaction
            if not document_results and interaction_id:
                print(
                    f"🔍 No specific questions found, getting ALL document content for interaction {interaction_id}"
                )

                # Get all document context for this interaction
                async with AsyncSessionLocal() as db:
                    from app.models.context import DocumentContext
                    from sqlalchemy import select, and_

                    result = await db.execute(
                        select(DocumentContext).where(
                            and_(
                                DocumentContext.user_id == user_id,
                                DocumentContext.interaction_id == interaction_id,
                            )
                        )
                    )
                    doc_contexts = result.scalars().all()

                    for doc_ctx in doc_contexts:
                        # Extract all questions from the document
                        if doc_ctx.question_mapping:
                            for (
                                q_num,
                                question_text,
                            ) in doc_ctx.question_mapping.items():
                                answer = doc_ctx.answer_key.get(
                                    q_num, "Answer not provided"
                                )
                                document_results.append(
                                    {
                                        "id": f"doc_all_{q_num}",
                                        "media_id": doc_ctx.media_id,
                                        "document_type": doc_ctx.document_type,
                                        "content": f"Question {q_num}: {question_text}\n\nAnswer: {answer}",
                                        "main_topics": doc_ctx.main_topics or [],
                                        "total_questions": len(
                                            doc_ctx.question_mapping
                                        ),
                                        "relevance_score": 0.9,  # High relevance for all questions
                                        "question_number": q_num,
                                        "subject_area": doc_ctx.subject_area
                                        or "unknown",
                                        "difficulty_level": doc_ctx.difficulty_level
                                        or "unknown",
                                        "key_concepts": doc_ctx.key_concepts or [],
                                    }
                                )
                        else:
                            # If no question mapping, use the full content
                            document_results.append(
                                {
                                    "id": f"doc_full_{doc_ctx.media_id}",
                                    "media_id": doc_ctx.media_id,
                                    "document_type": doc_ctx.document_type,
                                    "content": doc_ctx.full_content
                                    or "Document content not available",
                                    "main_topics": doc_ctx.main_topics or [],
                                    "total_questions": doc_ctx.total_questions or 0,
                                    "relevance_score": 0.8,
                                    "subject_area": doc_ctx.subject_area or "unknown",
                                    "difficulty_level": doc_ctx.difficulty_level
                                    or "unknown",
                                    "key_concepts": doc_ctx.key_concepts or [],
                                }
                            )

            if document_results:
                metadata["sources_used"].append("document_content")
                metadata["retrieval_times"]["document_content"] = (
                    datetime.now() - start_time
                ).total_seconds()
                metadata["context_lengths"]["document_content"] = sum(
                    len(result["content"]) for result in document_results
                )
                print(f"✅ Found {len(document_results)} document results for context")
                return document_results

            # Fallback to document search using the integration service
            search_results = await document_integration_service.search_documents(
                user_id=user_id, query=message, interaction_id=interaction_id, top_k=5
            )

            if search_results:
                # Convert search results to expected format
                document_results = []
                for result in search_results:
                    document_results.append(
                        {
                            "id": result["id"],
                            "media_id": result.get("id", "unknown"),
                            "document_type": result.get("document_type", "unknown"),
                            "content": result["content"],
                            "main_topics": result.get("main_topics", []),
                            "total_questions": 1,  # Each chunk represents part of a document
                            "relevance_score": result.get("score", 0.8),
                            "question_number": result.get("question_number"),
                            "subject_area": result.get("subject_area", "unknown"),
                            "difficulty_level": result.get(
                                "difficulty_level", "unknown"
                            ),
                            "key_concepts": result.get("key_concepts", []),
                        }
                    )

                metadata["sources_used"].append("document_content")
                metadata["retrieval_times"]["document_content"] = (
                    datetime.now() - start_time
                ).total_seconds()
                metadata["context_lengths"]["document_content"] = sum(
                    len(result["content"]) for result in document_results
                )
                return document_results

            # Fallback to original database-based approach
            async with AsyncSessionLocal() as db:
                # Get document context for the interaction
                if interaction_id:
                    result = await db.execute(
                        select(DocumentContext).where(
                            and_(
                                DocumentContext.interaction_id == interaction_id,
                                DocumentContext.user_id == user_id,
                            )
                        )
                    )
                    document_contexts = result.scalars().all()
                else:
                    # Get recent document contexts for the user
                    result = await db.execute(
                        select(DocumentContext)
                        .where(DocumentContext.user_id == user_id)
                        .order_by(desc(DocumentContext.created_at))
                        .limit(3)
                    )
                    document_contexts = result.scalars().all()

                document_results = []
                for doc_ctx in document_contexts:
                    # Check if the message references specific questions in this document
                    question_numbers = self._extract_question_numbers(message)
                    relevant_content = self._extract_relevant_document_content(
                        doc_ctx, message, question_numbers
                    )

                    if relevant_content:
                        document_results.append(
                            {
                                "id": doc_ctx.id,
                                "media_id": doc_ctx.media_id,
                                "document_type": doc_ctx.document_type,
                                "content": relevant_content,
                                "main_topics": doc_ctx.main_topics,
                                "total_questions": doc_ctx.total_questions,
                                "relevance_score": 0.9,  # High relevance for document content
                            }
                        )

                # Update metadata
                metadata["sources_used"].append("document_content")
                metadata["retrieval_times"]["document_content"] = (
                    datetime.now() - start_time
                ).total_seconds()
                metadata["context_lengths"]["document_content"] = sum(
                    len(r.get("content", "")) for r in document_results
                )

                return document_results

        except Exception as e:
            print(f"⚠️ Document context retrieval error: {e}")
            metadata["sources_ignored"].append("document_content")
            metadata["retrieval_times"]["document_content"] = (
                datetime.now() - start_time
            ).total_seconds()
            return []

    async def _get_cross_interaction_context(
        self, user_id: str, interaction_id: Optional[str], message: str, metadata: Dict
    ) -> List[Dict[str, Any]]:
        """Get context from related interactions across the user's history"""
        start_time = datetime.now()

        try:
            async with AsyncSessionLocal() as db:
                # Get user learning profile
                result = await db.execute(
                    select(UserLearningProfile).where(
                        UserLearningProfile.user_id == user_id
                    )
                )
                learning_profile = result.scalar_one_or_none()

                if not learning_profile:
                    return []

                # Get related interactions
                related_interaction_ids = learning_profile.related_interactions or []
                if not related_interaction_ids:
                    return []

                # Get semantic summaries from related interactions
                result = await db.execute(
                    select(Interaction).where(
                        and_(
                            Interaction.id.in_(related_interaction_ids),
                            Interaction.user_id == user_id,
                            Interaction.id
                            != interaction_id,  # Exclude current interaction
                        )
                    )
                )
                related_interactions = result.scalars().all()

                cross_interaction_results = []
                for interaction in related_interactions:
                    if interaction.semantic_summary:
                        summary_data = interaction.semantic_summary

                        # Check if topics are relevant to current message
                        if self._is_relevant_to_message(summary_data, message):
                            cross_interaction_results.append(
                                {
                                    "interaction_id": interaction.id,
                                    "title": interaction.title,
                                    "summary": summary_data.get("updated_summary", ""),
                                    "key_topics": summary_data.get("key_topics", []),
                                    "relevance_score": 0.7,  # Medium relevance for cross-interaction
                                }
                            )

                # Update metadata
                metadata["sources_used"].append("cross_interaction")
                metadata["retrieval_times"]["cross_interaction"] = (
                    datetime.now() - start_time
                ).total_seconds()
                metadata["context_lengths"]["cross_interaction"] = sum(
                    len(r.get("summary", "")) for r in cross_interaction_results
                )

                return cross_interaction_results

        except Exception as e:
            metadata["sources_ignored"].append("cross_interaction")
            metadata["retrieval_times"]["cross_interaction"] = (
                datetime.now() - start_time
            ).total_seconds()
            return []

    async def _get_related_conversations(
        self, user_id: str, interaction_id: Optional[str], message: str, metadata: Dict
    ) -> List[Dict[str, Any]]:
        """Get context from related conversations within the same interaction"""
        start_time = datetime.now()

        try:
            if not interaction_id:
                return []

            async with AsyncSessionLocal() as db:
                # Get recent conversations from the same interaction
                result = await db.execute(
                    select(Conversation)
                    .where(
                        and_(
                            Conversation.interaction_id == interaction_id,
                            Conversation.role == "AI",  # Get AI responses for context
                        )
                    )
                    .order_by(desc(Conversation.created_at))
                    .limit(3)
                )
                conversations = result.scalars().all()

                related_conversations = []
                for conv in conversations:
                    if conv.content and isinstance(conv.content, dict):
                        content = conv.content.get("_result", {}).get("note", "")
                        if (
                            content and len(content) > 20
                        ):  # Only include substantial content
                            related_conversations.append(
                                {
                                    "id": conv.id,
                                    "content": content,
                                    "created_at": (
                                        conv.created_at.isoformat()
                                        if conv.created_at
                                        else ""
                                    ),
                                    "relevance_score": 0.8,  # High relevance for same interaction
                                }
                            )

                # Update metadata
                metadata["sources_used"].append("related_conversations")
                metadata["retrieval_times"]["related_conversations"] = (
                    datetime.now() - start_time
                ).total_seconds()
                metadata["context_lengths"]["related_conversations"] = sum(
                    len(r.get("content", "")) for r in related_conversations
                )

                return related_conversations

        except Exception as e:
            metadata["sources_ignored"].append("related_conversations")
            metadata["retrieval_times"]["related_conversations"] = (
                datetime.now() - start_time
            ).total_seconds()
            return []

    def _extract_question_numbers(self, message: str) -> List[int]:
        """Extract question numbers from the message"""
        question_numbers = []
        patterns = [
            r"question\s+(\d+)",
            r"problem\s+(\d+)",
            r"mcq\s+(\d+)",
            r"equation\s+(\d+)",
            r"exercise\s+(\d+)",
            r"task\s+(\d+)",
            r"(\d+)\.\s*$",  # Number at end of line
            r"explain\s+(?:the\s+)?(?:equation|question|problem|mcq)\s+(\d+)",  # "explain equation 6"
            r"what\s+is\s+(?:the\s+)?(?:equation|question|problem|mcq)\s+(\d+)",  # "what is equation 6"
            r"solve\s+(?:the\s+)?(?:equation|question|problem|mcq)\s+(\d+)",  # "solve equation 6"
        ]

        for pattern in patterns:
            matches = re.findall(pattern, message, re.IGNORECASE)
            for match in matches:
                try:
                    question_numbers.append(int(match))
                except ValueError:
                    continue

        # Remove duplicates while preserving order
        seen = set()
        unique_question_numbers = []
        for num in question_numbers:
            if num not in seen:
                seen.add(num)
                unique_question_numbers.append(num)

        return unique_question_numbers

    def _extract_relevant_document_content(
        self, doc_ctx: DocumentContext, message: str, question_numbers: List[int]
    ) -> str:
        """Extract relevant content from document context based on message and question numbers"""
        if not doc_ctx.full_content:
            return ""

        # If specific question numbers are mentioned, try to find those questions
        if question_numbers:
            print(
                f"🎯 Context Service: Looking for question numbers: {question_numbers}"
            )
            content_parts = []
            for q_num in question_numbers:
                # Try multiple patterns to find the question
                patterns = [
                    rf"{q_num}\.?\s+.*?(?=\d+\.|$)",  # Standard numbered pattern
                    rf"(?:question|problem|equation|exercise|task)\s+{q_num}.*?(?=\d+\.|$)",  # With type prefix
                    rf"{q_num}\.?\s+.*?(?=\n\d+\.|\n\n|\Z)",  # More flexible ending
                ]

                found_question = False
                for pattern in patterns:
                    match = re.search(
                        pattern, doc_ctx.full_content, re.IGNORECASE | re.DOTALL
                    )
                    if match:
                        content_parts.append(
                            f"Question {q_num}: {match.group(0).strip()}"
                        )
                        print(
                            f"✅ Context Service: Found question {q_num} using pattern"
                        )
                        found_question = True
                        break

                # If no specific pattern found, try to find any content with the number
                if not found_question:
                    # Look for any line containing the number
                    lines = doc_ctx.full_content.split("\n")
                    for line in lines:
                        if re.search(rf"\b{q_num}\b", line):
                            content_parts.append(f"Question {q_num}: {line.strip()}")
                            print(
                                f"✅ Context Service: Found question {q_num} using line search"
                            )
                            break

            if content_parts:
                return "\n\n".join(content_parts)

        # If no specific questions, return summary or key concepts
        if doc_ctx.content_summary:
            return doc_ctx.content_summary
        elif doc_ctx.key_concepts:
            return f"Key concepts: {', '.join(doc_ctx.key_concepts[:5])}"
        else:
            # Return first part of content
            return (
                doc_ctx.full_content[:500] + "..."
                if len(doc_ctx.full_content) > 500
                else doc_ctx.full_content
            )

    def _is_relevant_to_message(self, summary_data: Dict, message: str) -> bool:
        """Check if semantic summary is relevant to the current message"""
        if not summary_data:
            return False

        # Check if any key topics match words in the message
        key_topics = summary_data.get("key_topics", [])
        message_lower = message.lower()

        for topic in key_topics:
            if topic.lower() in message_lower:
                return True

        # Check if the summary contains relevant terms
        summary_text = summary_data.get("updated_summary", "").lower()
        if any(word in summary_text for word in message_lower.split() if len(word) > 3):
            return True

        return False

    async def _rank_and_optimize_context(
        self, context_sources: Dict, message: str, max_length: int
    ) -> Dict[str, Any]:
        """Rank and optimize context based on relevance and length constraints"""
        ranked_context = {
            "semantic_summary": context_sources["semantic_summary"],
            "vector_search": [],
            "document_content": [],
            "cross_interaction": [],
            "related_conversations": [],
        }

        current_length = len(context_sources["semantic_summary"])

        # Prioritize vector search results (most relevant)
        for result in context_sources["vector_search"]:
            if current_length + len(result.get("content", "")) < max_length:
                ranked_context["vector_search"].append(result)
                current_length += len(result.get("content", ""))

        # Add document content (high relevance for specific questions)
        for result in context_sources["document_content"]:
            if current_length + len(result.get("content", "")) < max_length:
                ranked_context["document_content"].append(result)
                current_length += len(result.get("content", ""))

        # Add related conversations (contextual relevance)
        for result in context_sources["related_conversations"]:
            if current_length + len(result.get("content", "")) < max_length:
                ranked_context["related_conversations"].append(result)
                current_length += len(result.get("content", ""))

        # Add cross-interaction context (lower priority)
        for result in context_sources["cross_interaction"]:
            if current_length + len(result.get("summary", "")) < max_length:
                ranked_context["cross_interaction"].append(result)
                current_length += len(result.get("summary", ""))

        return ranked_context

    def _build_final_context(self, ranked_context: Dict, message: str) -> str:
        """Build the final context string from ranked sources"""
        context_parts = []

        # Add semantic summary first (always include if available)
        if ranked_context["semantic_summary"]:
            context_parts.append(ranked_context["semantic_summary"])

        # Add vector search results
        if ranked_context["vector_search"]:
            vector_parts = []
            for result in ranked_context["vector_search"]:
                content = result.get("content", "")
                if content:
                    vector_parts.append(f"**Previous Discussion:** {content}")
            if vector_parts:
                context_parts.append("\n".join(vector_parts))

        # Add document content
        if ranked_context["document_content"]:
            doc_parts = []
            for result in ranked_context["document_content"]:
                content = result.get("content", "")
                if content:
                    doc_parts.append(f"**Document Content:** {content}")
            if doc_parts:
                context_parts.append("\n".join(doc_parts))

        # Add related conversations
        if ranked_context["related_conversations"]:
            conv_parts = []
            for result in ranked_context["related_conversations"]:
                content = result.get("content", "")
                if content:
                    conv_parts.append(f"**Related Discussion:** {content}")
            if conv_parts:
                context_parts.append("\n".join(conv_parts))

        # Add cross-interaction context
        if ranked_context["cross_interaction"]:
            cross_parts = []
            for result in ranked_context["cross_interaction"]:
                summary = result.get("summary", "")
                if summary:
                    cross_parts.append(f"**Related Topic:** {summary}")
            if cross_parts:
                context_parts.append("\n".join(cross_parts))

        return "\n\n".join(context_parts)

    def _generate_cache_key(
        self, user_id: str, interaction_id: Optional[str], message: str
    ) -> str:
        """Generate cache key for context retrieval"""
        key_components = [
            user_id,
            interaction_id or "",
            message[:100],
        ]  # Limit message length
        key_string = "|".join(key_components)
        return f"context:{hashlib.md5(key_string.encode()).hexdigest()}"

    async def log_context_usage(
        self,
        user_id: str,
        interaction_id: str,
        conversation_id: Optional[str],
        context_sources_used: List[str],
        context_sources_ignored: List[str],
        retrieval_time: float,
        user_query: str,
        query_type: str,
        response_quality: Optional[str] = None,
    ):
        """Log context usage for monitoring and optimization"""
        try:
            async with AsyncSessionLocal() as db:
                usage_log = ContextUsageLog(
                    interaction_id=interaction_id,
                    user_id=user_id,
                    conversation_id=conversation_id,
                    context_sources_used=context_sources_used,
                    context_sources_ignored=context_sources_ignored,
                    context_retrieval_time=retrieval_time,
                    user_query=user_query,
                    query_type=query_type,
                    response_quality=response_quality,
                    created_at=datetime.now(),
                )

                db.add(usage_log)
                await db.commit()

        except Exception as e:
            # Log error but don't fail the main operation
            print(f"Failed to log context usage: {e}")


# Global instance
context_service = ContextRetrievalService()
